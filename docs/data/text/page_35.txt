NISTAI100-1 AIRMF1.0
Table3: Categoriesandsubcategoriesforthe MEASURE function. (Continued)
Categories Subcategories
MEASURE 2.6: The AI system is evaluated regularly for safety
risks–asidentifiedintheMAPfunction. TheAIsystemtobede-
ployed is demonstrated to be safe, its residual negative risk does
notexceedtherisktolerance,anditcanfailsafely,particularlyif
made to operate beyond its knowledge limits. Safety metrics re-
flect system reliability and robustness, real-time monitoring, and
responsetimesforAIsystemfailures.
MEASURE 2.7: AI system security and resilience – as identified
inthe MAP function–areevaluatedanddocumented.
MEASURE 2.8: Risks associatedwith transparency andaccount-
ability – as identified in the MAP function – are examined and
documented.
MEASURE 2.9: The AI model is explained, validated, and docu-
mented, and AI system output is interpreted within its context –
asidentifiedinthe MAP function–toinformresponsibleuseand
governance.
MEASURE 2.10: Privacy risk of the AI system – as identified in
the MAP function–isexaminedanddocumented.
MEASURE 2.11: Fairness and bias – as identified in the MAP
function–areevaluatedandresultsaredocumented.
MEASURE 2.12: Environmental impact and sustainability of AI
model training and management activities – as identified in the
MAP function–areassessedanddocumented.
MEASURE 2.13: Effectiveness of the employed TEVV met-
rics and processes in the MEASURE function are evaluated and
documented.
MEASURE 3: MEASURE 3.1: Approaches, personnel, and documentation are
Mechanismsfor in place to regularly identify and track existing, unanticipated,
trackingidentified and emergent AI risks based on factors such as intended and ac-
AIrisksovertime tualperformanceindeployedcontexts.
areinplace.
MEASURE 3.2: Risk tracking approaches are considered for
settings where AI risks are difficult to assess using currently
available measurement techniques or where metrics are not yet
available.
Continuedonnextpage
Page30